# ğŸ§  Consulta tus documentos PDF con IA local (RAG + Streamlit)

Este proyecto permite subir un archivo PDF y hacerle preguntas en lenguaje natural, utilizando un sistema **RAG (Retrieval-Augmented Generation)** totalmente local. El modelo de lenguaje se ejecuta mediante **Ollama**, usando **LangChain** como framework y **Streamlit** como interfaz web.

ğŸ“½ï¸ **Este cÃ³digo fue presentado en un taller prÃ¡ctico** sobre sistemas RAG en local, en colaboraciÃ³n con **MarÃ­a JesÃºs Puerta Angulo** y el Club de los Frikis ğŸ”¥.

ğŸ”— Mira el taller completo aquÃ­:
ğŸ‘‰ [https://www.youtube.com/watch?v=i8m2kKP0bSQ](https://www.youtube.com/watch?v=i8m2kKP0bSQ)

---

## ğŸš€ Â¿QuÃ© hace esta app?

* Permite **subir un documento PDF**.
* Divide el contenido en **chunks solapados** (fragmentos).
* Genera **embeddings** usando un modelo local (Mistral vÃ­a Ollama).
* Almacena los vectores en **FAISS** para recuperaciÃ³n eficiente.
* Usa un modelo LLM local para **responder preguntas** basÃ¡ndose en el contenido real del documento.
* Todo con una interfaz amigable vÃ­a **Streamlit**.

---

## âš™ï¸ Requisitos

* Python 3.10+
* Ollama instalado y corriendo el modelo `mistral`
* Dependencias de Python:

```bash
pip install streamlit langchain faiss-cpu
```

> Nota: Se recomienda instalar tambiÃ©n `langchain-community` si no estÃ¡ incluido en tu entorno.

---

## ğŸ§© Estructura del cÃ³digo

### Carga y preprocesado del PDF

```python
loader = PyPDFLoader(ruta_pdf)
paginas = loader.load()

splitter = RecursiveCharacterTextSplitter(
    chunk_size=300,
    chunk_overlap=80,
    separators=["\n\n", "\n", ".", " "]
)
documentos = splitter.split_documents(paginas)
```

Se divide el texto en fragmentos superpuestos, lo que ayuda a que el modelo tenga suficiente contexto sin perder coherencia.

---

### Embeddings y base vectorial

```python
embeddings = OllamaEmbeddings(model="mistral")
vectorstore = FAISS.from_documents(documentos, embeddings)
```

Se genera una base vectorial para poder recuperar los fragmentos mÃ¡s relevantes a cada pregunta.

---

### Consulta y respuesta

```python
cadena_qa = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever(),
    return_source_documents=True,
    chain_type_kwargs={"prompt": plantilla}
)
```

El sistema responde con precisiÃ³n usando solo la informaciÃ³n del documento subido, gracias al modelo local y a la plantilla personalizada.

---

## ğŸ–¥ï¸ Ejecutar la app

1. Inicia tu servidor Ollama con el modelo `mistral` cargado:

```bash
ollama run mistral
```

2. Lanza la app:

```bash
streamlit run app_rag.py
```

3. Accede en tu navegador a `http://localhost:8501`, sube tu PDF y haz preguntas.

---

## ğŸ§ª Demo del Taller

ğŸ” Este proyecto fue parte del taller **"CÃ³mo instalar un sistema RAG en local paso a paso"**, presentado junto al Club de los Frikis.

ğŸ¬ **[Ver el video del taller en YouTube](https://www.youtube.com/watch?v=i8m2kKP0bSQ)**

---

## ğŸ“© Contacto

Â¿Dudas o quieres adaptar este sistema a tus propios documentos o dominio?
ContÃ¡ctame en [LinkedIn](https://www.linkedin.com/in/matias-palomino-luna24/) o deja un comentario en el video.
